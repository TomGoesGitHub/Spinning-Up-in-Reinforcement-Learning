import tensorflow as tf
import numpy as np

from .agent import Agent
from ..utils.common import calculate_returns, MdpDataset

class ReinforceAgent(Agent):
    '''
    To be more precice this is an implementation of the GPOMDP-agorithm with baseline,
    which is an improved version (less variance in gradient estimation) of classical REINFORCE.
    '''
    def __init__(self, environment, policy, policy_network, train_freq, gamma=0.99, baseline=True, callbacks=[]):
        super().__init__(environment, policy, gamma, callbacks)
        self.policy_network = policy_network
        self.train_freq = train_freq
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
        self.baseline = baseline
        self.memory = MdpDataset()

    def act(self, observation, training=False):
        policy_input = self.policy_network(observation)
        action = self.policy.act(policy_input, training)
        return action
    
    def _trigger_train_step(self, step_index):
        return step_index % self.train_freq == 0
    
    def _handle_memory(self, triggered_training):
        if triggered_training:
            self.memory.clear() # on-policy (clear memory after each update)
    
    def calculate_rollout_log_probability(self, log_probas, dones):
        rollout_log_probas = tf.scan(lambda a,x: x[0] + a * (1-x[1]),
                                    elems=(log_probas, dones),
                                    initializer=0.,
                                    reverse=True)
        return rollout_log_probas

    @tf.function
    def update_network(self, observations, actions, rewards, dones):
        # returns-to-go
        previous_dones = tf.concat([[1], dones[:-1]], axis=0)
        returns_to_go = tf.scan(lambda a,x: x[0] + self.gamma*a*(1-x[1]),
                                elems=(rewards, dones),
                                initializer=0.,
                                reverse=True)
        
        discounts = tf.scan(lambda a,x: 1.*x + self.gamma*a*(1-x), elems=previous_dones)

        with tf.GradientTape(persistent=True) as tape:
            # action probabilities
            policy_inputs = self.policy_network(observations)
            probas = self.policy.proba(policy_inputs, actions)
            log_probas = tf.math.log(probas)
            
            # # baseline
            # if self.baseline:
            #     numerator = log_probas**2 * discounts * returns_to_go
            #     denominator = log_probas**2 
            #     baseline = numerator / denominator
            # else:
            #     baseline = 0
            # baseline = tf.stop_gradient(baseline)
            
            objective = tf.reduce_mean(discounts * returns_to_go * log_probas)
            loss = (-1) * objective
        
        gradients = tape.gradient(target=loss, sources=self.policy_network.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))

    
    def _train_step(self, step_index):
        observations, actions, _, rewards, terminals, truncations = self.memory.get_all()       
        dones = tf.clip_by_value(terminals + truncations, 0, 1) 
        
        # drop non-terminated trajectories
        is_complete = tf.scan(lambda a,x: 1. if x[0] else (0. if x[1] else a),
                              elems=(terminals, truncations),
                              initializer=0.,
                              reverse=True)
        observations, actions, rewards, dones = [tf.boolean_mask(x, is_complete)
                                                 for x in (observations, actions, rewards, dones)]
        
        self.update_network(observations, actions, rewards, dones)
        

            
